{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(arr):\n",
    "    for idx in range(len(arr)):\n",
    "        img = arr[idx]\n",
    "        img = np.array(tf.image.random_brightness(img,max_delta=0.3))\n",
    "        while (img.mean() > 210) or (img.mean() < 50):\n",
    "            img = np.array(tf.image.random_brightness(img,max_delta=0.3))\n",
    "        arr[idx] = img\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2776, 1: 2398, 2: 2358, 3: 2448, 4: 5272}\n"
     ]
    }
   ],
   "source": [
    "path = '../../../data/processed/color80/'\n",
    "d = {}\n",
    "for idx, l in enumerate(['center', 'down', 'left', 'right', 'up']):\n",
    "    subpath = os.path.join(path, l)\n",
    "    files = glob.glob(subpath + '/*.jpg')\n",
    "    data = []\n",
    "    for f in files:\n",
    "        img = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n",
    "        data.append(img)\n",
    "    d[idx] = np.array(data)\n",
    "shapes = {key:val.shape[0] for key,val in d.items()}\n",
    "max_size = max(shapes.values())\n",
    "print(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5272, 1: 5272, 2: 5272, 3: 5272, 4: 5272}\n"
     ]
    }
   ],
   "source": [
    "for key, val in shapes.items():\n",
    "    diff = max_size - val\n",
    "    if diff <= 0:\n",
    "        continue\n",
    "    else:\n",
    "        smpl_idxs = np.random.randint(0, val-1, size=diff)\n",
    "        samples = augment(d[key][smpl_idxs])\n",
    "        d[key] = np.concatenate([d[key],samples])\n",
    "shapes = {key:val.shape[0] for key,val in d.items()}\n",
    "print(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "for key, val in d.items():\n",
    "    images.extend(list(val))\n",
    "    labels.extend([key]*len(val))\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, shuffle=True, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "mobile = tf.keras.applications.mobilenet_v2.MobileNetV2(include_top=False,weights='imagenet',input_shape=(80,80,3))\n",
    "for i in range(len(mobile.layers)):\n",
    "    mobile.layers[i].trainable = False\n",
    "\n",
    "# for i in range(-11,0):\n",
    "#     mobile.layers[i].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.Input(shape=(80, 80, 3))\n",
    "x = keras.layers.experimental.preprocessing.Rescaling(1./255)(input_layer)\n",
    "x = mobile(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(.2)(x)\n",
    "x = tf.keras.layers.Dense(640, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(320, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(160, activation='relu')(x)\n",
    "output_layer = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"mobile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobile\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 80, 80, 3)]       0         \n",
      "_________________________________________________________________\n",
      "rescaling_2 (Rescaling)      (None, 80, 80, 3)         0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Functi (None, 3, 3, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 640)               819840    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 320)               205120    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 160)               51360     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 805       \n",
      "=================================================================\n",
      "Total params: 3,335,109\n",
      "Trainable params: 1,077,125\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "model.compile(optimizer=opt, \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('./best_mobile.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "callbacks = [es,mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "186/186 [==============================] - 49s 253ms/step - loss: 0.6082 - accuracy: 0.7767\n",
      "Epoch 2/10\n",
      "186/186 [==============================] - 50s 268ms/step - loss: 0.2897 - accuracy: 0.8991\n",
      "Epoch 3/10\n",
      "186/186 [==============================] - 54s 293ms/step - loss: 0.2155 - accuracy: 0.9248\n",
      "Epoch 4/10\n",
      "186/186 [==============================] - 53s 285ms/step - loss: 0.1837 - accuracy: 0.9361\n",
      "Epoch 5/10\n",
      "186/186 [==============================] - 48s 256ms/step - loss: 0.1588 - accuracy: 0.9436\n",
      "Epoch 6/10\n",
      "186/186 [==============================] - 50s 269ms/step - loss: 0.1465 - accuracy: 0.9480\n",
      "Epoch 7/10\n",
      "186/186 [==============================] - 49s 266ms/step - loss: 0.1255 - accuracy: 0.9551\n",
      "Epoch 8/10\n",
      "186/186 [==============================] - 49s 266ms/step - loss: 0.1204 - accuracy: 0.9565\n",
      "Epoch 9/10\n",
      "186/186 [==============================] - 49s 265ms/step - loss: 0.1065 - accuracy: 0.9610\n",
      "Epoch 10/10\n",
      "186/186 [==============================] - 48s 258ms/step - loss: 0.1019 - accuracy: 0.9638\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  x=X_train,\n",
    "    y=y_train,\n",
    "  epochs=epochs, \n",
    "  batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcb1730d520>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_history = pd.DataFrame(history.history)\n",
    "train_history.loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "186/186 [==============================] - 47s 250ms/step - loss: 0.0698 - accuracy: 0.9742\n",
      "Epoch 2/2\n",
      "186/186 [==============================] - 46s 245ms/step - loss: 0.0690 - accuracy: 0.9737\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  x=X_train,\n",
    "    y=y_train,\n",
    "  epochs=2, \n",
    "  batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 5s 65ms/step - loss: 0.0611 - accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.061071157455444336, 0.977617621421814]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./eyenet/assets\n"
     ]
    }
   ],
   "source": [
    "# model.save('./eyenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(arr):\n",
    "    for idx in range(len(arr)):\n",
    "        img = arr[idx]\n",
    "        img = np.array(tf.image.random_brightness(img,max_delta=0.3))\n",
    "        while (img.mean() > 210) or (img.mean() < 50):\n",
    "            img = np.array(tf.image.random_brightness(img,max_delta=0.3))\n",
    "        arr[idx] = img\n",
    "    return arr\n",
    "\n",
    "def load_calibration_data(path, balance:bool=True):\n",
    "    d = {}\n",
    "    for idx, l in enumerate(['center', 'down', 'left', 'right', 'up']):\n",
    "        subpath = os.path.join(path, l)\n",
    "        files = glob.glob(subpath + '/*.jpg')\n",
    "        data = []\n",
    "        for f in files:\n",
    "            img = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n",
    "            data.append(img)\n",
    "        d[idx] = np.array(data)\n",
    "    shapes = {key:val.shape[0] for key,val in d.items()}\n",
    "    max_size = max(shapes.values())\n",
    "   \n",
    "    if balance == True:\n",
    "        for key, val in shapes.items():\n",
    "            diff = max_size - val\n",
    "            if diff <= 0:\n",
    "                continue\n",
    "            else:\n",
    "                smpl_idxs = np.random.randint(0, val-1, size=diff)\n",
    "                samples = augment(d[key][smpl_idxs])\n",
    "                d[key] = np.concatenate([d[key],samples])\n",
    "        shapes = {key:val.shape[0] for key,val in d.items()}\n",
    "        \n",
    "    images = []\n",
    "    labels = []\n",
    "    for key, val in d.items():\n",
    "        images.extend(list(val))\n",
    "        labels.extend([key]*len(val))\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    print('output dataset size: ',shapes)\n",
    "    return images, labels\n",
    "\n",
    "def load_model_for_tuning(path):\n",
    "    print('loading model')\n",
    "    model = tf.keras.models.load_model(path)\n",
    "    for i in range(len(model.layers)):\n",
    "        model.layers[i].trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    return model\n",
    "\n",
    "def tune_model(model, X, y):\n",
    "    model.fit(x= X, y=y, epochs=10, batch_size =32, shuffle=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dataset size:  {0: 414, 1: 414, 2: 414, 3: 414, 4: 414}\n",
      "loading model\n",
      "Epoch 1/10\n",
      "65/65 [==============================] - 8s 72ms/step - loss: 1.0521 - accuracy: 0.7145\n",
      "Epoch 2/10\n",
      "65/65 [==============================] - 5s 74ms/step - loss: 0.3205 - accuracy: 0.8981\n",
      "Epoch 3/10\n",
      "65/65 [==============================] - 5s 78ms/step - loss: 0.2094 - accuracy: 0.9343\n",
      "Epoch 4/10\n",
      "65/65 [==============================] - 5s 84ms/step - loss: 0.1651 - accuracy: 0.9435\n",
      "Epoch 5/10\n",
      "65/65 [==============================] - 5s 82ms/step - loss: 0.1457 - accuracy: 0.9469\n",
      "Epoch 6/10\n",
      "65/65 [==============================] - 5s 81ms/step - loss: 0.1234 - accuracy: 0.9580\n",
      "Epoch 7/10\n",
      "65/65 [==============================] - 5s 76ms/step - loss: 0.1007 - accuracy: 0.9686\n",
      "Epoch 8/10\n",
      "65/65 [==============================] - 5s 76ms/step - loss: 0.0819 - accuracy: 0.9715\n",
      "Epoch 9/10\n",
      "65/65 [==============================] - 5s 79ms/step - loss: 0.0845 - accuracy: 0.9715\n",
      "Epoch 10/10\n",
      "65/65 [==============================] - 5s 77ms/step - loss: 0.0643 - accuracy: 0.9807\n",
      "65/65 [==============================] - 5s 74ms/step - loss: 0.0335 - accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "X, y = load_calibration_data('../../../data/processed/color80_test/')\n",
    "eyenet = load_model_for_tuning('./eyenet/')\n",
    "eyenet_tuned = tune_model(eyenet, X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9807692307692307, 1: 0.9975903614457832, 2: 0.9987908101571946, 3: 0.9829683698296837, 4: 0.9963811821471653}\n"
     ]
    }
   ],
   "source": [
    "predictions = eyenet_tuned.predict(X)\n",
    "predictions =[i.argmax() for i in predictions]\n",
    "report = classification_report(labels,predictions, output_dict=True)\n",
    "report= pd.DataFrame(report).transpose()\n",
    "scores = report['f1-score'].iloc[0:5]\n",
    "scores = {int(i):j for i,j in zip(scores.index,scores.values)}\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = eyenet_tuned.predict(X)\n",
    "output = [(i.argmax(),i.max()) for i in predictions]\n",
    "output = pd.DataFrame(output, columns=['prediction','probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['true'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    414\n",
       "0      1\n",
       "4      1\n",
       "Name: true, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[(output.prediction==1)].true.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9050410316529895, 1: 0.9390681003584229, 2: 0.974910394265233, 3: 0.9311639549436797, 4: 0.950859950859951}\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(images)\n",
    "predictions =[i.argmax() for i in predictions]\n",
    "report = classification_report(labels,predictions, output_dict=True)\n",
    "report= pd.DataFrame(report).transpose()\n",
    "scores = report['f1-score'].iloc[0:5]\n",
    "scores = {int(i):j for i,j in zip(scores.index,scores.values)}\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [1,5,2,3,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr[3]+=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 2, 3.5, 6]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(basepath):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label in ['up','down','left','right','center']:\n",
    "        files = glob.glob(os.path.join(basepath,label) + '/*.jpg')\n",
    "        for f in files:\n",
    "            img = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n",
    "            img = img.reshape((50,50,1))\n",
    "#             img = np.expand_dims(img,0)\n",
    "            data.append(img)\n",
    "            labels.append(label)\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    print('loading complete.')\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading complete.\n"
     ]
    }
   ],
   "source": [
    "X, y = load_dataset(basepath='../../../data/raw/eye_frames/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15252, 50, 50, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['center', 'down', 'left', 'right', 'up'], dtype='<U6')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.33, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
