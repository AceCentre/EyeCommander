{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 5\n",
    "img_height = 50\n",
    "img_width = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36280 files belonging to 5 classes.\n",
      "Using 25396 files for training.\n",
      "Found 36280 files belonging to 5 classes.\n",
      "Using 10884 files for validation.\n",
      "['center', 'down', 'left', 'right', 'up']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  '../../../data/processed/eye_frames/',\n",
    "  validation_split=0.3,\n",
    "    color_mode=\"grayscale\",\n",
    "    label_mode='int',\n",
    "  subset=\"training\",\n",
    "    shuffle=True,\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  '../../../data/processed/eye_frames/',\n",
    "  validation_split=0.3,\n",
    "  color_mode=\"grayscale\",\n",
    "    label_mode='int',\n",
    "    shuffle=True,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"jupiter3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 50, 50, 1)]       0         \n",
      "_________________________________________________________________\n",
      "rescaling_3 (Rescaling)      (None, 50, 50, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 50, 50, 32)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 25, 25, 64)        2112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 128)       8320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,198,789\n",
      "Trainable params: 1,198,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = keras.Input(shape=(img_height, img_width, 1))\n",
    "x = keras.layers.experimental.preprocessing.Rescaling(1./255)(input_layer)\n",
    "x = keras.layers.Conv2D(32, 1, padding='same', activation='relu')(x)\n",
    "x = keras.layers.MaxPooling2D()(x)\n",
    "x = keras.layers.Conv2D(64, 1, padding='same', activation='relu')(x)\n",
    "x = keras.layers.MaxPooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Conv2D(128, 1, padding='same', activation='relu')(x)\n",
    "x = keras.layers.MaxPooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "output_layer = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"jupiter3\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0008, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "# opt = tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9)\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "model.compile(optimizer=opt, \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "   \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('./best_model_aug.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "callbacks = [es,mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 1.2901 - accuracy: 0.6123\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79872, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 1.2899 - accuracy: 0.6125 - val_loss: 1.1101 - val_accuracy: 0.7987\n",
      "Epoch 2/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 1.0712 - accuracy: 0.8364\n",
      "Epoch 00002: val_accuracy improved from 0.79872 to 0.87980, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 1.0711 - accuracy: 0.8365 - val_loss: 1.0274 - val_accuracy: 0.8798\n",
      "Epoch 3/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 1.0255 - accuracy: 0.8816\n",
      "Epoch 00003: val_accuracy improved from 0.87980 to 0.88619, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 26s 40ms/step - loss: 1.0255 - accuracy: 0.8816 - val_loss: 1.0227 - val_accuracy: 0.8862\n",
      "Epoch 4/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 1.0045 - accuracy: 0.9017\n",
      "Epoch 00004: val_accuracy improved from 0.88619 to 0.91595, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 1.0046 - accuracy: 0.9016 - val_loss: 0.9911 - val_accuracy: 0.9160\n",
      "Epoch 5/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9975 - accuracy: 0.9077\n",
      "Epoch 00005: val_accuracy did not improve from 0.91595\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 0.9977 - accuracy: 0.9075 - val_loss: 1.0085 - val_accuracy: 0.8974\n",
      "Epoch 6/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9843 - accuracy: 0.9212\n",
      "Epoch 00006: val_accuracy improved from 0.91595 to 0.94321, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 0.9842 - accuracy: 0.9213 - val_loss: 0.9644 - val_accuracy: 0.9432\n",
      "Epoch 7/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9713 - accuracy: 0.9349\n",
      "Epoch 00007: val_accuracy did not improve from 0.94321\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9713 - accuracy: 0.9349 - val_loss: 0.9658 - val_accuracy: 0.9409\n",
      "Epoch 8/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.9685 - accuracy: 0.9364\n",
      "Epoch 00008: val_accuracy improved from 0.94321 to 0.95507, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 38ms/step - loss: 0.9685 - accuracy: 0.9364 - val_loss: 0.9513 - val_accuracy: 0.9551\n",
      "Epoch 9/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9604 - accuracy: 0.9447\n",
      "Epoch 00009: val_accuracy did not improve from 0.95507\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 0.9604 - accuracy: 0.9446 - val_loss: 0.9640 - val_accuracy: 0.9426\n",
      "Epoch 10/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9578 - accuracy: 0.9477\n",
      "Epoch 00010: val_accuracy did not improve from 0.95507\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 0.9577 - accuracy: 0.9478 - val_loss: 0.9530 - val_accuracy: 0.9516\n",
      "Epoch 11/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.9547 - accuracy: 0.9504\n",
      "Epoch 00011: val_accuracy did not improve from 0.95507\n",
      "640/640 [==============================] - 25s 38ms/step - loss: 0.9547 - accuracy: 0.9504 - val_loss: 0.9549 - val_accuracy: 0.9497\n",
      "Epoch 12/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9536 - accuracy: 0.9513\n",
      "Epoch 00012: val_accuracy did not improve from 0.95507\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 0.9536 - accuracy: 0.9513 - val_loss: 0.9583 - val_accuracy: 0.9477\n",
      "Epoch 13/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9503 - accuracy: 0.9543\n",
      "Epoch 00013: val_accuracy improved from 0.95507 to 0.96408, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 38ms/step - loss: 0.9503 - accuracy: 0.9544 - val_loss: 0.9421 - val_accuracy: 0.9641\n",
      "Epoch 14/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.9509 - accuracy: 0.9537\n",
      "Epoch 00014: val_accuracy improved from 0.96408 to 0.96545, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 38ms/step - loss: 0.9509 - accuracy: 0.9537 - val_loss: 0.9399 - val_accuracy: 0.9654\n",
      "Epoch 15/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9499 - accuracy: 0.9549\n",
      "Epoch 00015: val_accuracy improved from 0.96545 to 0.96898, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9498 - accuracy: 0.9550 - val_loss: 0.9367 - val_accuracy: 0.9690\n",
      "Epoch 16/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9456 - accuracy: 0.9597\n",
      "Epoch 00016: val_accuracy did not improve from 0.96898\n",
      "640/640 [==============================] - 25s 38ms/step - loss: 0.9456 - accuracy: 0.9596 - val_loss: 0.9444 - val_accuracy: 0.9602\n",
      "Epoch 17/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9496 - accuracy: 0.9551\n",
      "Epoch 00017: val_accuracy did not improve from 0.96898\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 0.9496 - accuracy: 0.9551 - val_loss: 0.9425 - val_accuracy: 0.9629\n",
      "Epoch 18/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9480 - accuracy: 0.9567\n",
      "Epoch 00018: val_accuracy did not improve from 0.96898\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 0.9479 - accuracy: 0.9567 - val_loss: 0.9403 - val_accuracy: 0.9636\n",
      "Epoch 19/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9471 - accuracy: 0.9572\n",
      "Epoch 00019: val_accuracy did not improve from 0.96898\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9471 - accuracy: 0.9572 - val_loss: 0.9390 - val_accuracy: 0.9674\n",
      "Epoch 20/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9444 - accuracy: 0.9604\n",
      "Epoch 00020: val_accuracy improved from 0.96898 to 0.97001, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 38ms/step - loss: 0.9444 - accuracy: 0.9605 - val_loss: 0.9349 - val_accuracy: 0.9700\n",
      "Epoch 21/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9448 - accuracy: 0.9606\n",
      "Epoch 00021: val_accuracy did not improve from 0.97001\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9447 - accuracy: 0.9606 - val_loss: 0.9362 - val_accuracy: 0.9690\n",
      "Epoch 22/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9449 - accuracy: 0.9594\n",
      "Epoch 00022: val_accuracy improved from 0.97001 to 0.97172, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9449 - accuracy: 0.9594 - val_loss: 0.9341 - val_accuracy: 0.9717\n",
      "Epoch 23/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9426 - accuracy: 0.9624\n",
      "Epoch 00023: val_accuracy improved from 0.97172 to 0.97274, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9426 - accuracy: 0.9624 - val_loss: 0.9322 - val_accuracy: 0.9727\n",
      "Epoch 24/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.9416 - accuracy: 0.9631\n",
      "Epoch 00024: val_accuracy did not improve from 0.97274\n",
      "640/640 [==============================] - 26s 40ms/step - loss: 0.9416 - accuracy: 0.9631 - val_loss: 0.9402 - val_accuracy: 0.9649\n",
      "Epoch 25/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.9444 - accuracy: 0.9601\n",
      "Epoch 00025: val_accuracy did not improve from 0.97274\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9444 - accuracy: 0.9601 - val_loss: 0.9395 - val_accuracy: 0.9656\n",
      "Epoch 26/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9400 - accuracy: 0.9645\n",
      "Epoch 00026: val_accuracy did not improve from 0.97274\n",
      "640/640 [==============================] - 24s 38ms/step - loss: 0.9399 - accuracy: 0.9646 - val_loss: 0.9443 - val_accuracy: 0.9602\n",
      "Epoch 27/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9435 - accuracy: 0.9611\n",
      "Epoch 00027: val_accuracy improved from 0.97274 to 0.97354, saving model to ./best_model_aug.h5\n",
      "640/640 [==============================] - 25s 38ms/step - loss: 0.9435 - accuracy: 0.9611 - val_loss: 0.9315 - val_accuracy: 0.9735\n",
      "Epoch 28/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9415 - accuracy: 0.9632\n",
      "Epoch 00028: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9415 - accuracy: 0.9632 - val_loss: 0.9382 - val_accuracy: 0.9660\n",
      "Epoch 29/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.9436 - accuracy: 0.9605\n",
      "Epoch 00029: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9436 - accuracy: 0.9605 - val_loss: 0.9332 - val_accuracy: 0.9724\n",
      "Epoch 30/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9375 - accuracy: 0.9676\n",
      "Epoch 00030: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9375 - accuracy: 0.9675 - val_loss: 0.9356 - val_accuracy: 0.9689\n",
      "Epoch 31/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9387 - accuracy: 0.9659\n",
      "Epoch 00031: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9387 - accuracy: 0.9659 - val_loss: 0.9336 - val_accuracy: 0.9721\n",
      "Epoch 32/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9364 - accuracy: 0.9686\n",
      "Epoch 00032: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9364 - accuracy: 0.9686 - val_loss: 0.9346 - val_accuracy: 0.9705\n",
      "Epoch 33/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9372 - accuracy: 0.9675\n",
      "Epoch 00033: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9372 - accuracy: 0.9676 - val_loss: 0.9432 - val_accuracy: 0.9617\n",
      "Epoch 34/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.9399 - accuracy: 0.9645\n",
      "Epoch 00034: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 38ms/step - loss: 0.9399 - accuracy: 0.9645 - val_loss: 0.9321 - val_accuracy: 0.9732\n",
      "Epoch 35/50\n",
      "639/640 [============================>.] - ETA: 0s - loss: 0.9390 - accuracy: 0.9657\n",
      "Epoch 00035: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9389 - accuracy: 0.9657 - val_loss: 0.9503 - val_accuracy: 0.9547\n",
      "Epoch 36/50\n",
      "640/640 [==============================] - ETA: 0s - loss: 0.9397 - accuracy: 0.9649\n",
      "Epoch 00036: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 39ms/step - loss: 0.9397 - accuracy: 0.9649 - val_loss: 0.9402 - val_accuracy: 0.9641\n",
      "Epoch 37/50\n",
      "638/640 [============================>.] - ETA: 0s - loss: 0.9376 - accuracy: 0.9673\n",
      "Epoch 00037: val_accuracy did not improve from 0.97354\n",
      "640/640 [==============================] - 25s 40ms/step - loss: 0.9376 - accuracy: 0.9673 - val_loss: 0.9408 - val_accuracy: 0.9642\n",
      "Epoch 00037: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81a8d9da30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=50, \n",
    "  batch_size =batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../models/jupiter4/assets\n"
     ]
    }
   ],
   "source": [
    "# model.save('../models/jupiter4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(path):\n",
    "    files = glob.glob(path + '/*.jpg')\n",
    "    count = 1\n",
    "    for file in files:\n",
    "        img = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "        img = np.array(tf.image.random_brightness(img,max_delta=0.3))\n",
    "        while (img.mean() > 210) or (img.mean() < 50):\n",
    "            img = np.array(tf.image.random_brightness(img,max_delta=0.3))\n",
    "        cv2.imwrite(os.path.join(path,f'aug{count}.jpg'),img)\n",
    "        count+=1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3377\n",
      "5832\n",
      "2971\n",
      "3035\n",
      "2930\n"
     ]
    }
   ],
   "source": [
    "for path in ['../../../data/processed/eye_frames/center/',\n",
    "             '../../../data/processed/eye_frames/up/',\n",
    "             '../../../data/processed/eye_frames/down/',\n",
    "             '../../../data/processed/eye_frames/right/',\n",
    "             '../../../data/processed/eye_frames/left/']:\n",
    "    augment(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "input must be at least 3-D, got shape[50,50] [Op:AdjustContrastv2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-3ba505caf2b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../data/fifty_fifty3/center/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-8a345dfad2b5>\u001b[0m in \u001b[0;36maugment\u001b[0;34m(path, brightness, contrast)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontrast\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mrandom_contrast\u001b[0;34m(image, lower, upper, seed)\u001b[0m\n\u001b[1;32m   1826\u001b[0m   \u001b[0;31m# Generate an a float in [lower, upper]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m   \u001b[0mcontrast_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36madjust_contrast\u001b[0;34m(images, contrast_factor)\u001b[0m\n\u001b[1;32m   1932\u001b[0m       \u001b[0mflt_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1934\u001b[0;31m     adjusted = gen_image_ops.adjust_contrastv2(\n\u001b[0m\u001b[1;32m   1935\u001b[0m         flt_images, contrast_factor=contrast_factor, name=name)\n\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_image_ops.py\u001b[0m in \u001b[0;36madjust_contrastv2\u001b[0;34m(images, contrast_factor, name)\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: input must be at least 3-D, got shape[50,50] [Op:AdjustContrastv2]"
     ]
    }
   ],
   "source": [
    "augment('../../../data/fifty_fifty3/center/', brightness=False, contrast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../../../data/fifty_fifty/center/aug2125.jpg', cv2.IMREAD_UNCHANGED)\n",
    "img2 = cv2.imread('../../../data/fifty_fifty/center/aug2148.jpg', cv2.IMREAD_UNCHANGED)\n",
    "img3 = cv2.imread('../../../data/fifty_fifty/center/aug2179.jpg', cv2.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./Models/jupiter1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('../data/eye_imgs/right/a1.jpg' ,cv2.IMREAD_UNCHANGED)\n",
    "img2 = cv2.imread('../data/eye_imgs/left/a10.jpg' ,cv2.IMREAD_UNCHANGED)\n",
    "# img = img.reshape((100,100,1))\n",
    "# img = np.expand_dims(img,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_both(left = img1, right = img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_prediction():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.expand_dims(img1,0)\n",
    "img2 = np.expand_dims(img2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.concatenate((img1,img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0141094, -0.907617 ,  3.9984462, -2.9328032,  1.9947188],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_both(left, right):\n",
    "    img1 = np.expand_dims(left,0)\n",
    "    img2 = np.expand_dims(right,0)\n",
    "    concat = np.concatenate((img1,img2))\n",
    "    classes = model.predict_classes(concat)\n",
    "    if classes[0] == classes[1]:\n",
    "        return classes[0]\n",
    "    else:\n",
    "        pred = model.predict(concat)\n",
    "        a = (pred[0].argmax(), pred[0].max())\n",
    "        b = (pred[1].argmax(), pred[1].max())\n",
    "        if a[1] > b[1]:\n",
    "            return a[0]\n",
    "        else:\n",
    "            return b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frame(self, eye_img):\n",
    "    img = eye_img.copy()\n",
    "    batch = np.expand_dims(img,0)\n",
    "    prediction = self.model.predict_classes(batch)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['center', 'down', 'left', 'right', 'up']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe44183d8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(img)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
