{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_classes = 5\n",
    "img_height = 50\n",
    "img_width = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29232 files belonging to 5 classes.\n",
      "Using 23386 files for training.\n",
      "Found 29232 files belonging to 5 classes.\n",
      "Using 5846 files for validation.\n",
      "['center', 'down', 'left', 'right', 'up']\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  '../../../data/fifty_fifty/',\n",
    "  validation_split=0.2,\n",
    "    color_mode=\"grayscale\",\n",
    "    label_mode='int',\n",
    "  subset=\"training\",\n",
    "    shuffle=True,\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  '../../../data/fifty_fifty/',\n",
    "  validation_split=0.2,\n",
    "  color_mode=\"grayscale\",\n",
    "    label_mode='int',\n",
    "    shuffle=True,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"jupiter3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 50, 50, 1)]       0         \n",
      "_________________________________________________________________\n",
      "rescaling_3 (Rescaling)      (None, 50, 50, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 50, 50, 32)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 25, 25, 64)        2112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 128)       8320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 2,372,869\n",
      "Trainable params: 2,372,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = keras.Input(shape=(img_height, img_width, 1))\n",
    "x = keras.layers.experimental.preprocessing.Rescaling(1./255)(input_layer)\n",
    "x = keras.layers.Conv2D(32, 1, padding='same', activation='relu')(x)\n",
    "x = keras.layers.MaxPooling2D()(x)\n",
    "x = keras.layers.Conv2D(64, 1, padding='same', activation='relu')(x)\n",
    "x = keras.layers.MaxPooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Conv2D(128, 1, padding='same', activation='relu')(x)\n",
    "x = keras.layers.MaxPooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "output_layer = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"jupiter3\")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "# opt = tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9)\n",
    "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "model.compile(optimizer=opt, \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "   \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('./best_model_aug.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "callbacks = [es,mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1424/1424 [==============================] - ETA: 0s - loss: 1.1276 - accuracy: 0.7777\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89621, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 48s 34ms/step - loss: 1.1276 - accuracy: 0.7777 - val_loss: 1.0115 - val_accuracy: 0.8962\n",
      "Epoch 2/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 1.0079 - accuracy: 0.8984\n",
      "Epoch 00002: val_accuracy improved from 0.89621 to 0.92290, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 49s 35ms/step - loss: 1.0079 - accuracy: 0.8984 - val_loss: 0.9835 - val_accuracy: 0.9229\n",
      "Epoch 3/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9793 - accuracy: 0.9257\n",
      "Epoch 00003: val_accuracy improved from 0.92290 to 0.93361, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 49s 34ms/step - loss: 0.9794 - accuracy: 0.9257 - val_loss: 0.9726 - val_accuracy: 0.9336\n",
      "Epoch 4/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9675 - accuracy: 0.9375\n",
      "Epoch 00004: val_accuracy improved from 0.93361 to 0.93607, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 49s 35ms/step - loss: 0.9676 - accuracy: 0.9374 - val_loss: 0.9700 - val_accuracy: 0.9361\n",
      "Epoch 5/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9620 - accuracy: 0.9425\n",
      "Epoch 00005: val_accuracy improved from 0.93607 to 0.95100, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 47s 33ms/step - loss: 0.9621 - accuracy: 0.9424 - val_loss: 0.9535 - val_accuracy: 0.9510\n",
      "Epoch 6/50\n",
      "1424/1424 [==============================] - ETA: 0s - loss: 0.9563 - accuracy: 0.9479\n",
      "Epoch 00006: val_accuracy improved from 0.95100 to 0.95627, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 51s 36ms/step - loss: 0.9563 - accuracy: 0.9479 - val_loss: 0.9501 - val_accuracy: 0.9563\n",
      "Epoch 7/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 0.9537 - accuracy: 0.9505\n",
      "Epoch 00007: val_accuracy improved from 0.95627 to 0.95926, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 44s 31ms/step - loss: 0.9537 - accuracy: 0.9505 - val_loss: 0.9456 - val_accuracy: 0.9593\n",
      "Epoch 8/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9522 - accuracy: 0.9526\n",
      "Epoch 00008: val_accuracy improved from 0.95926 to 0.96031, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 43s 30ms/step - loss: 0.9523 - accuracy: 0.9525 - val_loss: 0.9441 - val_accuracy: 0.9603\n",
      "Epoch 9/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9496 - accuracy: 0.9548\n",
      "Epoch 00009: val_accuracy did not improve from 0.96031\n",
      "1424/1424 [==============================] - 43s 30ms/step - loss: 0.9495 - accuracy: 0.9549 - val_loss: 0.9455 - val_accuracy: 0.9594\n",
      "Epoch 10/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 0.9475 - accuracy: 0.9568\n",
      "Epoch 00010: val_accuracy did not improve from 0.96031\n",
      "1424/1424 [==============================] - 48s 34ms/step - loss: 0.9474 - accuracy: 0.9568 - val_loss: 0.9476 - val_accuracy: 0.9566\n",
      "Epoch 11/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9478 - accuracy: 0.9562\n",
      "Epoch 00011: val_accuracy did not improve from 0.96031\n",
      "1424/1424 [==============================] - 52s 36ms/step - loss: 0.9478 - accuracy: 0.9562 - val_loss: 0.9514 - val_accuracy: 0.9531\n",
      "Epoch 12/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9454 - accuracy: 0.9593\n",
      "Epoch 00012: val_accuracy did not improve from 0.96031\n",
      "1424/1424 [==============================] - 52s 36ms/step - loss: 0.9454 - accuracy: 0.9594 - val_loss: 0.9529 - val_accuracy: 0.9526\n",
      "Epoch 13/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9454 - accuracy: 0.9592\n",
      "Epoch 00013: val_accuracy did not improve from 0.96031\n",
      "1424/1424 [==============================] - 49s 34ms/step - loss: 0.9454 - accuracy: 0.9592 - val_loss: 0.9467 - val_accuracy: 0.9580\n",
      "Epoch 14/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 0.9460 - accuracy: 0.9583\n",
      "Epoch 00014: val_accuracy improved from 0.96031 to 0.96242, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 49s 35ms/step - loss: 0.9459 - accuracy: 0.9583 - val_loss: 0.9425 - val_accuracy: 0.9624\n",
      "Epoch 15/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9454 - accuracy: 0.9593\n",
      "Epoch 00015: val_accuracy did not improve from 0.96242\n",
      "1424/1424 [==============================] - 51s 36ms/step - loss: 0.9454 - accuracy: 0.9593 - val_loss: 0.9465 - val_accuracy: 0.9587\n",
      "Epoch 16/50\n",
      "1424/1424 [==============================] - ETA: 0s - loss: 0.9484 - accuracy: 0.9564\n",
      "Epoch 00016: val_accuracy did not improve from 0.96242\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9484 - accuracy: 0.9564 - val_loss: 0.9427 - val_accuracy: 0.9619\n",
      "Epoch 17/50\n",
      "1424/1424 [==============================] - ETA: 0s - loss: 0.9472 - accuracy: 0.9571\n",
      "Epoch 00017: val_accuracy improved from 0.96242 to 0.96733, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 49s 35ms/step - loss: 0.9472 - accuracy: 0.9571 - val_loss: 0.9374 - val_accuracy: 0.9673\n",
      "Epoch 18/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 0.9444 - accuracy: 0.9601\n",
      "Epoch 00018: val_accuracy did not improve from 0.96733\n",
      "1424/1424 [==============================] - 49s 35ms/step - loss: 0.9444 - accuracy: 0.9601 - val_loss: 0.9446 - val_accuracy: 0.9594\n",
      "Epoch 19/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9430 - accuracy: 0.9617\n",
      "Epoch 00019: val_accuracy did not improve from 0.96733\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9430 - accuracy: 0.9617 - val_loss: 0.9478 - val_accuracy: 0.9571\n",
      "Epoch 20/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9416 - accuracy: 0.9633\n",
      "Epoch 00020: val_accuracy did not improve from 0.96733\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9416 - accuracy: 0.9633 - val_loss: 0.9439 - val_accuracy: 0.9605\n",
      "Epoch 21/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 0.9377 - accuracy: 0.9668\n",
      "Epoch 00021: val_accuracy improved from 0.96733 to 0.97067, saving model to ./best_model_aug.h5\n",
      "1424/1424 [==============================] - 48s 34ms/step - loss: 0.9377 - accuracy: 0.9668 - val_loss: 0.9336 - val_accuracy: 0.9707\n",
      "Epoch 22/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9395 - accuracy: 0.9650\n",
      "Epoch 00022: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9395 - accuracy: 0.9649 - val_loss: 0.9415 - val_accuracy: 0.9626\n",
      "Epoch 23/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9397 - accuracy: 0.9647\n",
      "Epoch 00023: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 49s 34ms/step - loss: 0.9397 - accuracy: 0.9647 - val_loss: 0.9376 - val_accuracy: 0.9677\n",
      "Epoch 24/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9410 - accuracy: 0.9637\n",
      "Epoch 00024: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 49s 34ms/step - loss: 0.9410 - accuracy: 0.9637 - val_loss: 0.9364 - val_accuracy: 0.9682\n",
      "Epoch 25/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 0.9413 - accuracy: 0.9632\n",
      "Epoch 00025: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9413 - accuracy: 0.9632 - val_loss: 0.9457 - val_accuracy: 0.9589\n",
      "Epoch 26/50\n",
      "1424/1424 [==============================] - ETA: 0s - loss: 0.9386 - accuracy: 0.9662\n",
      "Epoch 00026: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 49s 35ms/step - loss: 0.9386 - accuracy: 0.9662 - val_loss: 0.9480 - val_accuracy: 0.9568\n",
      "Epoch 27/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9358 - accuracy: 0.9687\n",
      "Epoch 00027: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9358 - accuracy: 0.9687 - val_loss: 0.9438 - val_accuracy: 0.9607\n",
      "Epoch 28/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 0.9377 - accuracy: 0.9667\n",
      "Epoch 00028: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 49s 35ms/step - loss: 0.9377 - accuracy: 0.9667 - val_loss: 0.9459 - val_accuracy: 0.9589\n",
      "Epoch 29/50\n",
      "1423/1424 [============================>.] - ETA: 0s - loss: 0.9397 - accuracy: 0.9650\n",
      "Epoch 00029: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9397 - accuracy: 0.9650 - val_loss: 0.9501 - val_accuracy: 0.9547\n",
      "Epoch 30/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9380 - accuracy: 0.9665\n",
      "Epoch 00030: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9380 - accuracy: 0.9665 - val_loss: 0.9352 - val_accuracy: 0.9691\n",
      "Epoch 31/50\n",
      "1422/1424 [============================>.] - ETA: 0s - loss: 0.9384 - accuracy: 0.9660\n",
      "Epoch 00031: val_accuracy did not improve from 0.97067\n",
      "1424/1424 [==============================] - 50s 35ms/step - loss: 0.9385 - accuracy: 0.9659 - val_loss: 0.9350 - val_accuracy: 0.9696\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbbbc342c40>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=50, \n",
    "  batch_size =batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../models/jupiter4/assets\n"
     ]
    }
   ],
   "source": [
    "# model.save('../models/jupiter4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_augmentation(images:list):\n",
    "    for idx, img in enumerate(images):\n",
    "        aug_img = tf.image.random_brightness(img,max_delta=0.2)\n",
    "        if (aug_img.mean() > 200) or (aug_img.mean() < 60):\n",
    "            continue\n",
    "        else:\n",
    "            images[idx] = aug_img\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(path, brightness:bool, contrast:bool):\n",
    "    files = glob.glob(path + '/*.jpg')\n",
    "    count = 1\n",
    "    for file in files:\n",
    "        img = cv2.imread(file, cv2.IMREAD_UNCHANGED)\n",
    "        if brightness == True:\n",
    "            img = tf.image.random_brightness(img,max_delta=0.2)\n",
    "        if contrast == True:\n",
    "            img = tf.image.random_contrast(img, 0.2, 0.5)\n",
    "        img = np.array(img)\n",
    "        if (img.mean() > 210) or (img.mean() < 50):\n",
    "            continue\n",
    "        cv2.imwrite(os.path.join(path,f'aug{count}.jpg'),img)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in ['../../../data/fifty_fifty/center/',\n",
    "             '../../../data/fifty_fifty/up/',\n",
    "             '../../../data/fifty_fifty/down/',\n",
    "             '../../../data/fifty_fifty/right/',\n",
    "             '../../../data/fifty_fifty/left/']:\n",
    "    augment(path, brightness=True, contrast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "input must be at least 3-D, got shape[50,50] [Op:AdjustContrastv2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-3ba505caf2b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../data/fifty_fifty3/center/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-8a345dfad2b5>\u001b[0m in \u001b[0;36maugment\u001b[0;34m(path, brightness, contrast)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontrast\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mrandom_contrast\u001b[0;34m(image, lower, upper, seed)\u001b[0m\n\u001b[1;32m   1826\u001b[0m   \u001b[0;31m# Generate an a float in [lower, upper]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m   \u001b[0mcontrast_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0madjust_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36madjust_contrast\u001b[0;34m(images, contrast_factor)\u001b[0m\n\u001b[1;32m   1932\u001b[0m       \u001b[0mflt_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1934\u001b[0;31m     adjusted = gen_image_ops.adjust_contrastv2(\n\u001b[0m\u001b[1;32m   1935\u001b[0m         flt_images, contrast_factor=contrast_factor, name=name)\n\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_image_ops.py\u001b[0m in \u001b[0;36madjust_contrastv2\u001b[0;34m(images, contrast_factor, name)\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: input must be at least 3-D, got shape[50,50] [Op:AdjustContrastv2]"
     ]
    }
   ],
   "source": [
    "augment('../../../data/fifty_fifty3/center/', brightness=False, contrast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../../../data/fifty_fifty/center/aug2125.jpg', cv2.IMREAD_UNCHANGED)\n",
    "img2 = cv2.imread('../../../data/fifty_fifty/center/aug2148.jpg', cv2.IMREAD_UNCHANGED)\n",
    "img3 = cv2.imread('../../../data/fifty_fifty/center/aug2179.jpg', cv2.IMREAD_UNCHANGED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./Models/jupiter1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('../data/eye_imgs/right/a1.jpg' ,cv2.IMREAD_UNCHANGED)\n",
    "img2 = cv2.imread('../data/eye_imgs/left/a10.jpg' ,cv2.IMREAD_UNCHANGED)\n",
    "# img = img.reshape((100,100,1))\n",
    "# img = np.expand_dims(img,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_both(left = img1, right = img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_prediction():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.expand_dims(img1,0)\n",
    "img2 = np.expand_dims(img2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.concatenate((img1,img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0141094, -0.907617 ,  3.9984462, -2.9328032,  1.9947188],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_both(left, right):\n",
    "    img1 = np.expand_dims(left,0)\n",
    "    img2 = np.expand_dims(right,0)\n",
    "    concat = np.concatenate((img1,img2))\n",
    "    classes = model.predict_classes(concat)\n",
    "    if classes[0] == classes[1]:\n",
    "        return classes[0]\n",
    "    else:\n",
    "        pred = model.predict(concat)\n",
    "        a = (pred[0].argmax(), pred[0].max())\n",
    "        b = (pred[1].argmax(), pred[1].max())\n",
    "        if a[1] > b[1]:\n",
    "            return a[0]\n",
    "        else:\n",
    "            return b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_frame(self, eye_img):\n",
    "    img = eye_img.copy()\n",
    "    batch = np.expand_dims(img,0)\n",
    "    prediction = self.model.predict_classes(batch)[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['center', 'down', 'left', 'right', 'up']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe44183d8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(img)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
